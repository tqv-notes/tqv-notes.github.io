---
title:  "Proximal Gradient Method"
mathjax: true
layout: post
categories: media
---

It is quite common in many optimization to consider the following problem:

$$
\underset{x}{\text{minimize}}~\{f(x) = g(x) + h(x)\}
$$

here, \\( f(x) \\) can be seen as total cost function, \\( g(x) \\) is the standard least squares cost function and \\( h(x) \\) is the regularization term.

If \\( f(x) \\) is a smooth convex function, there are many optimization techniques in the literature but we will mention here two most basic ones: Newton method and gradient descent method.

The ultimate goal of this note is to present a method for the case \\( h(x) \\) is nondifferentiable: the proximal gradient method.

## Newton method

Consider a smooth convex function \\( f(x) \\)

## Gradient descent method

## Proximal gradient method
